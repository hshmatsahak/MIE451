{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2-ZYhG2zn3qf"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IcqEP3fGkec9",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87cbbc4b-b616-4d83-e79a-bde8144289f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Extra imports\n",
        "from abc import abstractmethod, ABC\n",
        "import os\n",
        "import wget\n",
        "from pathlib import Path\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB, _BaseNB, _BaseDiscreteNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import random\n",
        "random.seed()\n",
        "from typing import Tuple, List, Optional, Union, Dict\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "\n",
        "# Please add necessary imports here\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Nq7O9NKnnorw"
      },
      "outputs": [],
      "source": [
        "import wget\n",
        "from pathlib import Path\n",
        "filename = wget.download(\"https://github.com/MIE451-1513-2023/course-datasets/raw/main/20_newsgroups.zip\", \"20_newsgroups.zip\")\n",
        "_ = wget.download(\"https://github.com/MIE451-1513-2023/course-datasets/raw/main/training_files_Q7.txt\", \"training_files_Q7.txt\")\n",
        "_ = wget.download(\"https://github.com/MIE451-1513-2023/course-datasets/raw/main/testing_files_Q7.txt\", \"testing_files_Q7.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uwVs1bGK64FB"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!unzip 20_newsgroups.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QgyLW2Lp69RD"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = \"20_newsgroups\"\n",
        "ALL_FILES = [pth for pth in Path(DATA_DIR).glob(\"**/*\") if pth.is_file() and not pth.name.startswith(\".\")]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "432hrnVOkedA"
      },
      "source": [
        "# Q7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSihMe6ekedB"
      },
      "source": [
        "## Q7(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwX8PxjCkedD"
      },
      "source": [
        "use the following code cell to implement your feature encoding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_file_text(text):\n",
        "    new_text = re.sub(\"Newsgroups:.*?\\n\", \"\", text)\n",
        "    new_text = re.sub(\"Xref:.*?\\n\", \"\", new_text)\n",
        "    new_text = re.sub(\"Path:.*?\\n\", \"\", new_text)\n",
        "    new_text = re.sub(\"Date:.*?\\n\", \"\", new_text)\n",
        "    new_text = re.sub(\"Followup-To:.*?\\n\", \"\", new_text)\n",
        "    new_text = re.sub(\"Lines:.*?\\n\", \"\", new_text)\n",
        "    new_text = re.sub(\"Reply-To:.*?\\n\", \"\", new_text)\n",
        "    new_text = re.sub(\"Message-ID:.*?\\n\", \"\", new_text)\n",
        "    new_text = re.sub(\"From:.*?\\n\", \"\", new_text)\n",
        "    new_text = re.sub(\"NNTP-Posting-Host:.*?\\n\", \"\", new_text)\n",
        "    return new_text"
      ],
      "metadata": {
        "id": "uCUd9BhUqIHh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def corpus_words(file_list):\n",
        "    data = []\n",
        "    for file_path in file_list:\n",
        "      with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n",
        "        file_data = file.read()\n",
        "        file_data = clean_file_text(file_data)\n",
        "        data.append(file_data)\n",
        "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
        "    # vectorizer = CountVectorizer(decode_error='ignore', lowercase=True, tokenizer=tokenizer.tokenize, stop_words='english', binary=True)\n",
        "    vectorizer = TfidfVectorizer(sublinear_tf=True, lowercase=True, tokenizer=tokenizer.tokenize, max_df=0.8, min_df=3, stop_words=\"english\", norm=\"l2\", binary=False, ngram_range=(1,1))\n",
        "    sparse_matrix = vectorizer.fit_transform(data)\n",
        "    return sparse_matrix"
      ],
      "metadata": {
        "id": "u_-tO3f5qLKj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topic_name(file_path):\n",
        "    return file_path.parent.name\n",
        "\n",
        "def get_target(topic_name):\n",
        "    topics = [\"talk.politics.mideast\", \"rec.autos\", \"comp.sys.mac.hardware\", \"alt.atheism\", \"rec.sport.baseball\",\n",
        "     \"comp.os.ms-windows.misc\", \"rec.sport.hockey\", \"sci.crypt\", \"sci.med\", \"talk.politics.misc\",\n",
        "     \"rec.motorcycles\", \"comp.windows.x\", \"comp.graphics\", \"comp.sys.ibm.pc.hardware\", \"sci.electronics\",\n",
        "     \"talk.politics.guns\", \"sci.space\", \"soc.religion.christian\", \"misc.forsale\", \"talk.religion.misc\"]\n",
        "    return topics.index(topic_name)"
      ],
      "metadata": {
        "id": "Y5n9-ZqyqRZX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus_words(ALL_FILES).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bOWWkhD0jui",
        "outputId": "59479e36-054c-4e71-a0b4-cd7221d5899f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(19997, 56178)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "aknEVgx0kedD"
      },
      "outputs": [],
      "source": [
        "def data_q7(file_list):\n",
        "    X = None\n",
        "    y = None\n",
        "\n",
        "    #Please remember to put index for your dataframe as the file name\n",
        "    #For example: pd.DataFrame(data, index=[str(f) for f in file_list], columns=[...])\n",
        "\n",
        "    X = pd.DataFrame.sparse.from_spmatrix(corpus_words(file_list), index=list(map(str, file_list)))\n",
        "\n",
        "    # Create a dataframe of targets (y)\n",
        "    y = [get_target(get_topic_name(file_path)) for file_path in file_list]\n",
        "\n",
        "    selector = SelectKBest(chi2, k=20000)\n",
        "    selector.fit_transform(X,y)\n",
        "    cols = selector.get_support(indices=True)\n",
        "    X = X.iloc[:,cols]\n",
        "\n",
        "    # validate return types\n",
        "    assert isinstance(X, pd.DataFrame) and isinstance(y, list), \"incorrect return types\"\n",
        "\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNRE4K4gkedE"
      },
      "source": [
        "## Q7(b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzXn77E0kedE"
      },
      "source": [
        "Use the following code cell to implement your model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gY3zH_yfkedE"
      },
      "outputs": [],
      "source": [
        "# for clf, name in (\n",
        "#     (LogisticRegression(C=5, max_iter=1000), \"Logistic Regression\"),\n",
        "#     (RidgeClassifier(alpha=1.0, solver=\"sparse_cg\"), \"Ridge Classifier\"),\n",
        "#     (KNeighborsClassifier(n_neighbors=100), \"kNN\"),\n",
        "#     (RandomForestClassifier(), \"Random Forest\"),\n",
        "#     # L2 penalty Linear SVC\n",
        "#     (LinearSVC(C=0.1, dual=False, max_iter=1000), \"Linear SVC\"),\n",
        "#     # L2 penalty Linear SGD\n",
        "#     (\n",
        "#         SGDClassifier(\n",
        "#             loss=\"log_loss\", alpha=1e-4, n_iter_no_change=3, early_stopping=True\n",
        "#         ),\n",
        "#         \"log-loss SGD\",\n",
        "#     ),\n",
        "#     # NearestCentroid (aka Rocchio classifier)\n",
        "#     (NearestCentroid(), \"NearestCentroid\"),\n",
        "#     # Sparse naive Bayes classifier\n",
        "#     (ComplementNB(alpha=0.1), \"Complement naive Bayes\"),\n",
        "# ):\n",
        "def build_model_q7():\n",
        "    # Write your code here, define your model and return it\n",
        "    MODELQ7 = MultinomialNB(alpha=0.01)\n",
        "    return MODELQ7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWKu6i31kedF"
      },
      "source": [
        "Code for evaluating p at k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "s4o8CXDJkedF"
      },
      "outputs": [],
      "source": [
        "from pandas.core.arrays import numpy_\n",
        "def calculate_average_precision_at_k(model_q7, data_func, all_files, training_files, testing_files, k=None):\n",
        "\n",
        "    training_files = [str(f) for f in open(training_files, mode='r').read().splitlines()]\n",
        "    testing_files = [str(f) for f in open(testing_files, mode='r').read().splitlines()]\n",
        "    if k is None:\n",
        "        k = len(testing_files)\n",
        "\n",
        "    X, y = data_func(all_files)\n",
        "    X[\"gt\"] = y\n",
        "    training = X.loc[training_files]\n",
        "    X_train = training.loc[:, training.columns!=\"gt\"]\n",
        "    y_train = training[\"gt\"].values\n",
        "\n",
        "    testing = X.loc[testing_files]\n",
        "    X_test = testing.loc[:, testing.columns!=\"gt\"]\n",
        "    y_test = testing[\"gt\"].values\n",
        "\n",
        "    model_q7.fit(X_train, y_train)\n",
        "    y_pred = model_q7.predict(X_test)\n",
        "    y_pred_prob = model_q7.predict_proba(X_test)\n",
        "    confidences = np.max(y_pred_prob, axis=1)\n",
        "\n",
        "    p_at_k = []\n",
        "    rel_at_k = []\n",
        "    confidence_order = np.argsort(confidences)\n",
        "    for i in range(1, k+1):\n",
        "        top_confidence = confidence_order[-i:]\n",
        "        pred_top_i = y_pred[top_confidence]\n",
        "        gt_top_i = np.array(y_test)[top_confidence]\n",
        "        p_at_i = np.sum(pred_top_i == gt_top_i) / i\n",
        "        rel_at_i = (pred_top_i[0] == gt_top_i[0])\n",
        "        p_at_k.append(p_at_i)\n",
        "        rel_at_k.append(rel_at_i)\n",
        "    print(f\"average precision at {k} is {np.dot(p_at_k, rel_at_k) / k}\")\n",
        "    return np.dot(p_at_k, rel_at_k) / k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QmicMF0ykedG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96ecaedb-29f6-4de8-cd2c-b026f1362805"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-bb8fdfbda1db>:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[\"gt\"] = y\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average precision at 4000 is 0.8773310138389259\n"
          ]
        }
      ],
      "source": [
        "# Example usage:\n",
        "######This line of code must be able to run on Google Colab in under 7 minutes.#####\n",
        "######Code that runs longer than 7 minutes on the autograder will receive 0 marks for Q7#####\n",
        "m = calculate_average_precision_at_k(build_model_q7(), data_q7, ALL_FILES, \"training_files_Q7.txt\", \"testing_files_Q7.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8y4vOO5kedH"
      },
      "source": [
        "# Q7(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXORTLuikedH"
      },
      "source": [
        "• A clear and concise description of your chosen feature set and feature encoding \\\n",
        "Feature encoding: tf-idf vector. I chose to set the arguments as sublinear_tf=True, lowercase=True, tokenizer=tokenizer.tokenize, max_df=0.8, min_df=3, stop_words=\"english\", norm=\"l2\", binary=False, ngram_range=(1,1). This was a result of empirically tuning hyperparameters using random search technique. \\\n",
        "Feature set: I used the SelectKBest function from sk-learn. I used the chi2 metric to determine the 20000 most relevant features to use as input to the classifier train_and_predict function. 20000 worked best empirically by a small margin, and allows us not to overfit to the train data. This gave superior performance to just using the most common words\n",
        "\n",
        "• The name of the classifier you chose \\\n",
        "Multinomial Naive Bayes\n",
        "\n",
        "• Why you chose the feature set, feature encoding, and classifier you used \\\n",
        "For feature encoding, i used tf-idf because it gives us not only the frequency of words for all documents, but also accounts for the relevance of that word to each topic. Tf-idf is well-used in information retrieval and is an efficient and widely taught method of creating sparse feature matrices, so I expect it to generalize well to different tasks and datasets. For feature set, i used SelectKBest with chi2 metric; because it was easy to implement (built-in scikit-learn method). chi2 worked best empirically over f_classif and mutual_info_classif, so I selected it. The number of features chosen (20,000) was also selected empirically. If we select all features, I fear we may overfit to words that wont appear in out-of-sample test sets.\n",
        "\n",
        "• The final AP performance that your choices attained. We will verify this score by running\n",
        "the model returned by your build model q7 function, and the data as returned by your\n",
        "data Q7 function. \\\n",
        "0.8773310138389259. Not good better than the baseline at least..."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eRnPLrm9dMI-"
      },
      "execution_count": 13,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}