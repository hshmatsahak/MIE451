{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRRUNdzazX0C"
      },
      "outputs": [],
      "source": [
        "!pip install whoosh\n",
        "!pip install pytrec_eval\n",
        "!pip install wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_U6kVg5VzX0F"
      },
      "outputs": [],
      "source": [
        "import wget\n",
        "wget.download(\"https://github.com/MIE451-1513-2023/course-datasets/raw/main/government.zip\", \"government.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uuwEo5gzX0H"
      },
      "outputs": [],
      "source": [
        "!unzip government.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n132137DzX0I"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "# Put all your imports here\n",
        "from whoosh import index, writing\n",
        "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
        "from whoosh.analysis import *\n",
        "from whoosh.qparser import QueryParser\n",
        "import os.path\n",
        "from pathlib import Path\n",
        "import tempfile\n",
        "import subprocess\n",
        "import pytrec_eval\n",
        "import wget\n",
        "import abc\n",
        "from abc import abstractmethod\n",
        "from whoosh.analysis import Filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zck_Qt3fzX0K"
      },
      "outputs": [],
      "source": [
        "class IRSystem(metaclass=abc.ABCMeta):\n",
        "    \"\"\"\n",
        "    Abstract class which is inherited by other IR system\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_dir):\n",
        "        # DON'T change the following definitions for topic_file, qrels_file, document_dir, file_list\n",
        "        self.topic_file = os.path.join(data_dir, \"gov.topics\")\n",
        "        self.qrels_file = os.path.join(data_dir, \"gov.qrels\")\n",
        "        self.document_dir = os.path.join(data_dir, \"documents\")\n",
        "        self.file_list = [str(filePath) for filePath in Path(self.document_dir).glob(\"**/*\") if filePath.is_file()]\n",
        "\n",
        "        self.create_index()\n",
        "        self.create_parser_searcher()\n",
        "\n",
        "    @abstractmethod\n",
        "    def create_index(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def add_files(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def create_parser_searcher(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def perform_search(self, topic_phrase):\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def post_process_score(score):\n",
        "        return score\n",
        "\n",
        "    @staticmethod\n",
        "    def print_trec_eval_result(results):\n",
        "\n",
        "        if not results:\n",
        "            print('empty results')\n",
        "            return\n",
        "\n",
        "        def print_line(name, scope, num):\n",
        "            print('{:25s}{:8s}{:.4f}'.format(name, scope, num))\n",
        "\n",
        "        for query_id, query_measures in results.items():\n",
        "            for measure, value in query_measures.items():\n",
        "                if measure == \"runid\":\n",
        "                    continue\n",
        "                print_line(measure, query_id, value)\n",
        "\n",
        "        for measure in query_measures.keys():\n",
        "            if measure == \"runid\":\n",
        "                continue\n",
        "            print_line(\n",
        "                measure,\n",
        "                'all',\n",
        "                pytrec_eval.compute_aggregated_measure(\n",
        "                    measure,\n",
        "                    [query_measures[measure]\n",
        "                     for query_measures in results.values()]))\n",
        "\n",
        "\n",
        "    def score(self,docnum,topic_results, topic_phrase):\n",
        "        return topic_results.score(docnum)\n",
        "\n",
        "\n",
        "    def print_rel_name(self, q_id):\n",
        "        with open(self.topic_file, \"r\") as tf:\n",
        "            topics = tf.read().splitlines()\n",
        "        for topic in topics:\n",
        "            topic_id, topic_phrase = tuple(topic.split(\" \", 1))\n",
        "            if topic_id == q_id:\n",
        "                print(\"---------------------------Topic_id and Topic_phrase----------------------------------\")\n",
        "                print(topic_id, topic_phrase)\n",
        "                 # get search result\n",
        "                topic_results = self.perform_search(topic_phrase)\n",
        "                print(\"---------------------------Return documents----------------------------------\")\n",
        "                for (docnum, result) in enumerate(topic_results):\n",
        "                    score = self.score(docnum, topic_results, topic_phrase)\n",
        "                    score = self.post_process_score(score)\n",
        "                    print(\"%s Q0 %s %d %lf test\" % (topic_id, os.path.basename(result[\"file_path\"]), docnum, score))\n",
        "                print(\"---------------------------Relevant documents----------------------------------\")\n",
        "                with open(self.qrels_file, 'r') as f_qrel:\n",
        "                    qrels = f_qrel.readlines()\n",
        "                    for i in qrels:\n",
        "                        qid, _, doc, rel = i.rstrip().split(\" \")\n",
        "                        if qid == q_id and rel == \"1\":\n",
        "                            print(i.rstrip())\n",
        "\n",
        "    def py_trec_eval(self):\n",
        "\n",
        "        self.create_parser_searcher()\n",
        "        # Load topic file - a list of topics(search phrases) used for evalutation\n",
        "        with open(self.topic_file, \"r\") as tf:\n",
        "            topics = tf.read().splitlines()\n",
        "\n",
        "            # create an output file to which we'll write our results\n",
        "        temp_output_file = tempfile.mkstemp()[1]\n",
        "        with open(temp_output_file, \"w\") as outputTRECFile:\n",
        "            # for each evaluated topic:\n",
        "            # build a query and record the results in the file in TREC_EVAL format\n",
        "            for topic in topics:\n",
        "                topic_id, topic_phrase = tuple(topic.split(\" \", 1))\n",
        "                # get search result\n",
        "                topic_results = self.perform_search(topic_phrase)\n",
        "                # format the result\n",
        "                for (docnum, result) in enumerate(topic_results):\n",
        "                    score = self.score(docnum, topic_results, topic_phrase)\n",
        "                    outputTRECFile.write(\n",
        "                        \"%s Q0 %s %d %lf test\\n\" % (topic_id, os.path.basename(result[\"file_path\"]), docnum, score))\n",
        "                    topic_with_result = topic_id\n",
        "\n",
        "\n",
        "        with open(self.qrels_file, 'r') as f_qrel:\n",
        "            qrel = pytrec_eval.parse_qrel(f_qrel)\n",
        "\n",
        "        with open(temp_output_file, 'r') as f_run:\n",
        "            run = pytrec_eval.parse_run(f_run)\n",
        "\n",
        "        evaluator = pytrec_eval.RelevanceEvaluator(\n",
        "            qrel, pytrec_eval.supported_measures)\n",
        "\n",
        "        results = evaluator.evaluate(run)\n",
        "\n",
        "        #fill results dictionary with queries that were returned 0 documents\n",
        "        topic_ids = {t.split()[0] for t in topics}\n",
        "        for emptyresult_topicid in topic_ids.difference(set(results.keys())):\n",
        "            num_rel = float(sum(qrel[emptyresult_topicid].values()))\n",
        "            if num_rel>0:\n",
        "              topic_stats={measure:0.0 for measure in results[topic_with_result]}\n",
        "            else:\n",
        "              topic_stats={measure:1.0 for measure in results[topic_with_result]}\n",
        "            topic_stats[\"num_rel\"]=num_rel\n",
        "            topic_stats[\"num_ret\"] = 0.0\n",
        "            topic_stats[\"num_rel_ret\"] = 0.0\n",
        "            topic_stats[\"num_q\"]=1.0\n",
        "\n",
        "            results[emptyresult_topicid] = topic_stats\n",
        "\n",
        "\n",
        "        self.print_trec_eval_result(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wL9yL8YBzX0M"
      },
      "outputs": [],
      "source": [
        "# Dont change this! Use it as-is in your code\n",
        "# This filter will run for both the index and the query\n",
        "class CustomFilter(Filter):\n",
        "    is_morph = True\n",
        "    def __init__(self, filterFunc, *args, **kwargs):\n",
        "        self.customFilter = filterFunc\n",
        "        self.args = args\n",
        "        self.kwargs = kwargs\n",
        "    def __eq__(self):\n",
        "        return (other\n",
        "                and self.__class__ is other.__class__)\n",
        "    def __call__(self, tokens):\n",
        "        for t in tokens:\n",
        "            if t.mode == 'query': # if called by query parser\n",
        "                t.text = self.customFilter(t.text, *self.args, **self.kwargs)\n",
        "                yield t\n",
        "            else: # == 'index' if called by indexer\n",
        "                t.text = self.customFilter(t.text, *self.args, **self.kwargs)\n",
        "                yield t\n",
        "\n",
        "# Dont change this! Use it as-is in your code if you rerank your results using a non-Whoosh scoring function\n",
        "class NeuralResults():\n",
        "  '''\n",
        "  This class is used to rerank documents returned by whoosh in an interface that\n",
        "  imitates whoosh.searching.Results (the datatype of topicResults in pyTrecEval)\n",
        "  '''\n",
        "  def __init__(self, booleansearchdocs,scores,rankings, file_list):\n",
        "    self.results=[]\n",
        "    if rankings.shape:\n",
        "      for idx in rankings:\n",
        "        self.results.append({'file_path':file_list[booleansearchdocs[idx]],'score':scores[idx] })\n",
        "\n",
        "  def score(self,docnum):\n",
        "    return self.results[docnum]['score']\n",
        "\n",
        "  def __iter__(self):\n",
        "    return self.results.__iter__()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfmjO1oGzX0N"
      },
      "source": [
        "## Question 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-W9UocAFzX0O"
      },
      "source": [
        "**1. The auto-grader will extract and use the following variables, DON'T change the their names:**\n",
        "\n",
        "      self.topic_file  \n",
        "      self.qrels_file  \n",
        "      self.document_dir   \n",
        "      self.file_list  \n",
        "      self.index_sys  \n",
        "      self.query_parser  \n",
        "      self.searcher   \n",
        "\n",
        "\n",
        "\n",
        "**2. DON'T change the names of the already defined funtions**  \n",
        "**3. DON'T change the py_trec_eval function**  \n",
        "**4. DON'T change the class names including CustomFilter, IRSystem, IRQ2, IRQ3, IRQ4**  \n",
        "**5. DON'T change the CustomFilter class and DON'T create any new custom filter class that is used to define Whoosh schema**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. If you are doing neural IR you should precompute your corpus embeddings and save them in the corpus_embeddings.json file. If you do this, please keep the code used to generate the embeddings somewhere in this notebook**"
      ],
      "metadata": {
        "id": "YSboZcTu1hag"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LH4OD-z6zX0Q"
      },
      "outputs": [],
      "source": [
        "class IRQ4(IRSystem):\n",
        "    def create_index(self):\n",
        "        \"\"\"\n",
        "        INPUT:\n",
        "            None\n",
        "        OUTPUT:\n",
        "            None\n",
        "\n",
        "        NOTE: Please update self.index_sys which should have type whoosh.index.FileIndex\n",
        "        \"\"\"\n",
        "        # DON't change the name of 'index_sys'\n",
        "        self.index_sys = None\n",
        "\n",
        "    def add_files(self):\n",
        "        \"\"\"\n",
        "        INPUT:\n",
        "            None\n",
        "        OUTPUT:\n",
        "            None\n",
        "\n",
        "        NOTE: Add buffer to self.index_sys\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def create_parser_searcher(self):\n",
        "        \"\"\"\n",
        "        INPUT:\n",
        "            None\n",
        "        OUTPUT:\n",
        "            None\n",
        "\n",
        "        NOTE: Please update self.query_parser and self.self.searcherwhich should have type whoosh.qparser.default.QueryParser and whoosh.searching.Searcher respectively\n",
        "        \"\"\"\n",
        "         # DON't change the names of 'query_parser' and 'searcher'\n",
        "        self.query_parser = None\n",
        "        self.searcher = None\n",
        "\n",
        "    def perform_search(self, topic_phrase):\n",
        "        \"\"\"\n",
        "        INPUT:\n",
        "            topic_phrase: string\n",
        "        OUTPUT:\n",
        "            topicResults: whoosh.searching.Results OR NeuralResults\n",
        "\n",
        "        NOTE: Utilize self.query_parser and self.searcher to calculate the result for topic_phrase\n",
        "        \"\"\"\n",
        "        topicResults = None\n",
        "        return topicResults"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVbZxmO5zX0R"
      },
      "source": [
        "### Please answer the following questions here\n",
        "(a) A clear list of all final modifications made.  \n",
        "(b)  Why each modification was made â€“ how did it help?  \n",
        "(c)  The  final  MAP  performance  that  these  modifications  attained."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLmXGmq-zX0T"
      },
      "source": [
        "### Q4 Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktz7dw5gzX0U"
      },
      "outputs": [],
      "source": [
        "q4 = IRQ4(\"government\")\n",
        "assert(isinstance(q4.index_sys, FileIndex)), \"Index Type\"\n",
        "assert(isinstance(q4.query_parser, QueryParser)), \"Query Parser Type\"\n",
        "assert(isinstance(q4.searcher, Searcher)), \"Searcher Type\"\n",
        "print(\"Q4 Types Validated\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
